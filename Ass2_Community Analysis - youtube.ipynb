{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e4f60bbd-4ea6-4ec9-9265-e2e3c6e03e22",
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import community  \n",
    "import json\n",
    "from langdetect import detect, LangDetectException\n",
    "import pandas as pd\n",
    "\n",
    "import json\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import community\n",
    "import random\n",
    "import csv\n",
    "\n",
    "import nltk\n",
    "import string\n",
    "import re\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "# https://medium.com/@monigrancharov/text-language-detection-with-python-beb49d9667b3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0dd04d32-524c-4516-8794-e2cf35b92f39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# jupyter notebook --NotebookApp.iopub_data_rate_limit=1.0e10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3f87f73e-aa22-4d62-ab71-69b98e2b5a68",
   "metadata": {},
   "outputs": [],
   "source": [
    "fJsonName_youtube = 'NEW_youtube_olympics_data_limited_250_per_hashtag.json'\n",
    "with open(fJsonName_youtube, 'r') as f:\n",
    "    ySubmissions = json.load(f)\n",
    "\n",
    "# List to store combined data for English posts only\n",
    "olympics_post2 = []\n",
    "\n",
    "for sub in ySubmissions:\n",
    "    try:\n",
    "        # Detect the language of both title and description\n",
    "        title_language = detect(sub['Video Title'])\n",
    "        description_language = detect(sub['Description'])\n",
    "\n",
    "        # Include the post only if both title and description are in English\n",
    "        if title_language == 'en' and description_language == 'en':\n",
    "            # Create a dictionary for the post itself\n",
    "            post_data = {\n",
    "                'title': sub['Video Title'],\n",
    "                'text': sub['Description'],\n",
    "                'olympics': sub['Hashtag'],\n",
    "                'author': sub['Video Tuthor'], \n",
    "                'comments': []  \n",
    "            }\n",
    "\n",
    "            # Iterate through comments and add only English comments\n",
    "            for com in sub['Comments']:\n",
    "                comment_text = com['text']\n",
    "                try:\n",
    "                    # Detect if the comment is in English\n",
    "                    comment_language = detect(comment_text)\n",
    "                    if comment_language == 'en':\n",
    "                        comment_data = {\n",
    "                            'author': com['author'],\n",
    "                            'text': comment_text,\n",
    "                            'created': com['published_at']\n",
    "                        }\n",
    "                        post_data['comments'].append(comment_data)\n",
    "                except LangDetectException:\n",
    "                    continue\n",
    "\n",
    "            # Append the post with its associated English comments to the combined_data list\n",
    "            olympics_post2.append(post_data)\n",
    "    except LangDetectException:\n",
    "        continue\n",
    "\n",
    "\n",
    "paris2024 = []\n",
    "tokyo2020 = []\n",
    "rio2016 = []\n",
    "\n",
    "\n",
    "for post in olympics_post2:\n",
    "    if post['olympics'] == \"#paris2024\":\n",
    "        paris2024.append(post)\n",
    "    elif post['olympics'] == \"#tokyo2020\":\n",
    "        tokyo2020.append(post)\n",
    "    elif post['olympics'] == \"#rio2016\":\n",
    "        rio2016.append(post)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0ecb96aa-52ac-431f-82ef-3c0182dfc7ab",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def write_graphml(graph, graphfilename):\n",
    "    nx.write_graphml(graph, graphfilename)\n",
    "    print(f\"Subgraph saved as {graphfilename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ac9b3e29-0cb0-455f-b69c-15d4324bac7b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def build_youtube_graph_video(subset):\n",
    "    youtubeG = nx.Graph()\n",
    "\n",
    "    # Iterate through each post in the dataset\n",
    "    for video in subset:\n",
    "        if video['title']:\n",
    "            # Add the video node with attributes\n",
    "            youtubeG.add_node(video['title'], type='video', hashtag=str(video['olympics']) if video['olympics'] else \"\")\n",
    "            \n",
    "            # Add nodes and edges for each comment author and link them to the corresponding video\n",
    "            for comment in video['comments']:\n",
    "                author = comment.get('author', '').strip()\n",
    "                    \n",
    "                if author:\n",
    "                    if not youtubeG.has_node(author):\n",
    "                        youtubeG.add_node(author, type='author')\n",
    "                        \n",
    "                    # Add a directed edge from author to the video they commented on\n",
    "                    youtubeG.add_edge(author, video['title'])\n",
    "                    \n",
    "    # Perform community detection using the Louvain method\n",
    "    partition = community.best_partition(youtubeG)\n",
    "\n",
    "    # Assign the community information as an attribute to each node\n",
    "    nx.set_node_attributes(youtubeG, partition, 'community')\n",
    "\n",
    "    # Return the graph with community detection information\n",
    "    return youtubeG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7d171a3c-8fe4-475c-a959-24639e03feb1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def build_youtube_graph_author(subset):\n",
    "    youtubeG = nx.Graph()\n",
    "\n",
    "    # Iterate through each post in the dataset\n",
    "    for video in subset:\n",
    "        if video['author']:\n",
    "            # Add the video node with attributes\n",
    "            youtubeG.add_node(video['author'], type='author', hashtag=str(video['olympics']) if video['olympics'] else \"\")\n",
    "            \n",
    "            # Add nodes and edges for each comment author and link them to the corresponding video\n",
    "            for comment in video['comments']:\n",
    "                author = comment.get('author', '').strip()\n",
    "                    \n",
    "                if author:\n",
    "                    if not youtubeG.has_node(author):\n",
    "                        youtubeG.add_node(author, type='author')\n",
    "                        \n",
    "                    # Add a directed edge from author to the video they commented on\n",
    "                    youtubeG.add_edge(author, video['author'])\n",
    "                    \n",
    "    # Perform community detection using the Louvain method\n",
    "    partition = community.best_partition(youtubeG)\n",
    "\n",
    "    # Assign the community information as an attribute to each node\n",
    "    nx.set_node_attributes(youtubeG, partition, 'community')\n",
    "\n",
    "    # Return the graph with community detection information\n",
    "    return youtubeG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "429c9934-2b55-4655-93e9-bd81a7a295e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_centrality_measures(graph, degree_threshold=1, betweenness_k=None):\n",
    "    graph = graph.copy()\n",
    "    if not graph.is_directed():\n",
    "        graph = graph.to_directed()\n",
    "\n",
    "     # Filter out nodes with a degree below the threshold\n",
    "    filtered_nodes = [n for n, d in graph.degree() if d < degree_threshold]\n",
    "    graph.remove_nodes_from(filtered_nodes)\n",
    "    \n",
    "    components = nx.weakly_connected_components(graph)\n",
    "    largest_component = max(components, key=len)\n",
    "    graph = graph.subgraph(largest_component).copy()  \n",
    "    try:\n",
    "        # Calculate various centrality measures\n",
    "        in_degree_centrality = nx.in_degree_centrality(graph)\n",
    "        degree_centrality = nx.degree_centrality(graph)\n",
    "        betweenness_centrality = nx.betweenness_centrality(graph, k=betweenness_k) if betweenness_k else {}\n",
    "        closeness_centrality = nx.closeness_centrality(graph)  # Works on undirected graphs\n",
    "        eigenvector_centrality = nx.eigenvector_centrality(graph, max_iter=5000)\n",
    "        \n",
    "        # Combine the centrality measures into a DataFrame\n",
    "        centrality_df = pd.DataFrame({\n",
    "            'in_degree_centrality': in_degree_centrality,\n",
    "            'degree_centrality':degree_centrality,\n",
    "            'betweenness_centrality': betweenness_centrality,\n",
    "            'closeness_centrality': closeness_centrality,\n",
    "            'eigenvector_centrality': eigenvector_centrality\n",
    "        })\n",
    "\n",
    "    \n",
    "    except nx.NetworkXError as e:\n",
    "        print(f\"Centrality Calculation Error: {e}\")\n",
    "        centrality_df = pd.DataFrame()\n",
    "        \n",
    "    return centrality_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "797d2279-dee9-4c65-be15-67006b843927",
   "metadata": {},
   "outputs": [],
   "source": [
    "# usage for different graphs- for aurhor\n",
    "author_youtubeG_olympic2024 = build_youtube_graph_author(paris2024)\n",
    "author_youtubeG_olympic2020 = build_youtube_graph_author(tokyo2020)\n",
    "author_youtubeG_olympic2016 = build_youtube_graph_author(rio2016)\n",
    "author_youtubeG_olympic = build_youtube_graph_author(olympics_post2)\n",
    "\n",
    "author_centrality_2024 = compute_centrality_measures(author_youtubeG_olympic2024, degree_threshold=5,betweenness_k=10)\n",
    "author_centrality_2020 = compute_centrality_measures(author_youtubeG_olympic2020, degree_threshold=5,betweenness_k=10)\n",
    "author_centrality_2016 = compute_centrality_measures(author_youtubeG_olympic2016, degree_threshold=5,betweenness_k=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "39810e20-211b-4c2b-877e-e6b076a3a307",
   "metadata": {},
   "outputs": [],
   "source": [
    "author_centrality_all = compute_centrality_measures(author_youtubeG_olympic, degree_threshold=5,betweenness_k=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "61b4be71-e7cc-4c0b-a801-f67031ea46f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# usage for different graphs- for video\n",
    "video_youtubeG_olympic2024 = build_youtube_graph_video(paris2024)\n",
    "video_youtubeG_olympic2020 = build_youtube_graph_video(tokyo2020)\n",
    "video_youtubeG_olympic2016 = build_youtube_graph_video(rio2016)\n",
    "video_youtubeG_olympic = build_youtube_graph_video(olympics_post2)\n",
    "\n",
    "video_centrality_2024 = compute_centrality_measures(video_youtubeG_olympic2024, degree_threshold=5,betweenness_k=10)\n",
    "video_centrality_2020 = compute_centrality_measures(video_youtubeG_olympic2020, degree_threshold=5,betweenness_k=10)\n",
    "video_centrality_2016 = compute_centrality_measures(video_youtubeG_olympic2016, degree_threshold=5,betweenness_k=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3f59975a-d3a1-429d-9bd2-7bede3236e10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# usage for different graphs- for video\n",
    "video_centrality_all = compute_centrality_measures(video_youtubeG_olympic, degree_threshold=5,betweenness_k=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "806d7157-c9e3-4ae7-97d7-8f5156b52485",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_max_betweenness_degree_subgraph(graph, centrality_df, output_filename):\n",
    "    max_betweenness_degree = centrality_df['betweenness_centrality'].idxmax()\n",
    "    print(f\"With highest betweenness-degree centrality: {max_betweenness_degree}\")\n",
    "    \n",
    "    # Get the subgraph of this author and their neighbors (immediate connections)\n",
    "    subgraph = graph.subgraph([max_betweenness_degree] + list(graph.neighbors(max_betweenness_degree)))\n",
    "    \n",
    "    # Save the subgraph as a GraphML file\n",
    "    nx.write_graphml(subgraph, output_filename)\n",
    "    print(f\"Subgraph saved as {output_filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ba1b0296-1ab3-474e-9dcc-17d8951ed9d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_max_in_degree_subgraph(graph, centrality_df, output_filename):\n",
    "    max_in_degree = centrality_df['in_degree_centrality'].idxmax()\n",
    "    print(f\"With highest in-degree centrality: {max_in_degree}\")\n",
    "    \n",
    "    subgraph = graph.subgraph([max_in_degree] + list(graph.neighbors(max_in_degree)))\n",
    "    \n",
    "    nx.write_graphml(subgraph, output_filename)\n",
    "    print(f\"Subgraph saved as {output_filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d50d8a54-c329-4631-b0b2-ba71614351bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With highest betweenness-degree centrality: Olympics\n",
      "Subgraph saved as betweenness_author_2024_ytb.graphml\n",
      "With highest betweenness-degree centrality: Olympics\n",
      "Subgraph saved as betweenness_author_2020_ytb.graphml\n",
      "With highest betweenness-degree centrality: Olympics\n",
      "Subgraph saved as betweenness_author_2016_ytb.graphml\n",
      "With highest betweenness-degree centrality: Olympics\n",
      "Subgraph saved as betweenness_author_olympic_ytb.graphml\n"
     ]
    }
   ],
   "source": [
    "#find the largest betweenness degree-for author\n",
    "extract_max_betweenness_degree_subgraph(author_youtubeG_olympic2024, author_centrality_2024, \"betweenness_author_2024_ytb.graphml\")\n",
    "extract_max_betweenness_degree_subgraph(author_youtubeG_olympic2020, author_centrality_2020, \"betweenness_author_2020_ytb.graphml\")\n",
    "extract_max_betweenness_degree_subgraph(author_youtubeG_olympic2016, author_centrality_2016, \"betweenness_author_2016_ytb.graphml\")\n",
    "extract_max_betweenness_degree_subgraph(author_youtubeG_olympic, author_centrality_all, \"betweenness_author_olympic_ytb.graphml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "56267ee5-6f39-45a6-9a40-1b1abfa03bdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With highest in-degree centrality: Olympics\n",
      "Subgraph saved as in_degree_author_2024_ytb.graphml\n",
      "With highest in-degree centrality: Olympics\n",
      "Subgraph saved as in_degree_author_2020_ytb.graphml\n",
      "With highest in-degree centrality: Olympics\n",
      "Subgraph saved as in_degree_author_2016_ytb.graphml\n",
      "With highest in-degree centrality: Olympics\n",
      "Subgraph saved as in_degree_author_olympic_ytb.graphml\n"
     ]
    }
   ],
   "source": [
    "#find the largestin-degree degree-for author\n",
    "extract_max_in_degree_subgraph(author_youtubeG_olympic2024, author_centrality_2024, \"in_degree_author_2024_ytb.graphml\")\n",
    "extract_max_in_degree_subgraph(author_youtubeG_olympic2020, author_centrality_2020, \"in_degree_author_2020_ytb.graphml\")\n",
    "extract_max_in_degree_subgraph(author_youtubeG_olympic2016, author_centrality_2016, \"in_degree_author_2016_ytb.graphml\")\n",
    "extract_max_in_degree_subgraph(author_youtubeG_olympic, author_centrality_all, \"in_degree_author_olympic_ytb.graphml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "356190a0-47d8-40f1-8e9a-9ba6f479ac9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With highest betweenness-degree centrality: THANK YOU PARIS! Closing Ceremony Highlights | #Paris2024\n",
      "Subgraph saved as betweenness_video_2024_ytb.graphml\n",
      "With highest betweenness-degree centrality: @francisnguyen6349\n",
      "Subgraph saved as betweenness_video_2020_ytb.graphml\n",
      "With highest betweenness-degree centrality: @Olympics\n",
      "Subgraph saved as betweenness_video_2016_ytb.graphml\n",
      "With highest betweenness-degree centrality: Men&#39;s 100m final 🏃‍♂️ | Tokyo Replays\n",
      "Subgraph saved as betweenness_video_olympic_ytb.graphml\n"
     ]
    }
   ],
   "source": [
    "#find the largest betweenness degree-for video\n",
    "extract_max_betweenness_degree_subgraph(video_youtubeG_olympic2024, video_centrality_2024, \"betweenness_video_2024_ytb.graphml\")\n",
    "extract_max_betweenness_degree_subgraph(video_youtubeG_olympic2020, video_centrality_2020, \"betweenness_video_2020_ytb.graphml\")\n",
    "extract_max_betweenness_degree_subgraph(video_youtubeG_olympic2016, video_centrality_2016, \"betweenness_video_2016_ytb.graphml\")\n",
    "extract_max_betweenness_degree_subgraph(video_youtubeG_olympic, video_centrality_all, \"betweenness_video_olympic_ytb.graphml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "756f8657-c929-4fd9-bd8f-0865ebb07a50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With highest in-degree centrality: Full Opening Ceremony ✨| Full Replay | Paris Replays\n",
      "Subgraph saved as in_degree_video_2024_ytb.graphml\n",
      "With highest in-degree centrality: 🇮🇳🥇 Neeraj Chopra wins historic gold for India | #Tokyo2020 Highlights\n",
      "Subgraph saved as in_degree_video_2020_ytb.graphml\n",
      "With highest in-degree centrality: Men&#39;s 100m Final | Rio 2016 Replay\n",
      "Subgraph saved as in_degree_video_2016_ytb.graphml\n",
      "With highest in-degree centrality: 🇮🇳🥇 Neeraj Chopra wins historic gold for India | #Tokyo2020 Highlights\n",
      "Subgraph saved as in_degree_video_olympic_ytb.graphml\n"
     ]
    }
   ],
   "source": [
    "#find the largestin-degree degree-for video\n",
    "extract_max_in_degree_subgraph(video_youtubeG_olympic2024, video_centrality_2024, \"in_degree_video_2024_ytb.graphml\")\n",
    "extract_max_in_degree_subgraph(video_youtubeG_olympic2020, video_centrality_2020, \"in_degree_video_2020_ytb.graphml\")\n",
    "extract_max_in_degree_subgraph(video_youtubeG_olympic2016, video_centrality_2016, \"in_degree_video_2016_ytb.graphml\")\n",
    "extract_max_in_degree_subgraph(video_youtubeG_olympic, video_centrality_all, \"in_degree_video_olympic_ytb.graphml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a9c50fd8-5594-460f-9289-1ade3af153c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subgraph saved as youtubeG_olympic_video.graphml\n",
      "Subgraph saved as youtubeG_olympic_author.graphml\n"
     ]
    }
   ],
   "source": [
    "#write the graphml for video and author\n",
    "youtubeG_olympic_video= write_graphml(video_youtubeG_olympic,\"youtubeG_olympic_video.graphml\")\n",
    "youtubeG_olympic_author = write_graphml(author_youtubeG_olympic,\"youtubeG_olympic_author.graphml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ab8c93ea-4e9d-41f9-b239-b37df530c770",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Author centrality Olympics\n",
      ":        in_degree_centrality  degree_centrality  betweenness_centrality  closeness_centrality  eigenvector_centrality\n",
      "count            476.000000         476.000000              476.000000            476.000000              476.000000\n",
      "mean               0.011835           0.023671                0.004013              0.357763                0.032035\n",
      "std                0.032613           0.065226                0.034050              0.055285                0.032816\n",
      "min                0.002105           0.004211                0.000000              0.203513                0.000126\n",
      "25%                0.006316           0.012632                0.000283              0.303224                0.011712\n",
      "50%                0.006316           0.012632                0.000531              0.388389                0.038242\n",
      "75%                0.008421           0.016842                0.001066              0.397989                0.041379\n",
      "max                0.614737           1.229474                0.717285              0.610540                0.621084\n",
      "Video centrality Olympics\n",
      ":        in_degree_centrality  degree_centrality  betweenness_centrality  closeness_centrality  eigenvector_centrality\n",
      "count           2356.000000        2356.000000             2356.000000           2356.000000             2356.000000\n",
      "mean               0.002839           0.005678                0.001177              0.265571                0.009960\n",
      "std                0.006521           0.013042                0.005825              0.021460                0.018039\n",
      "min                0.000425           0.000849                0.000000              0.162616                0.000001\n",
      "25%                0.001274           0.002548                0.000040              0.257265                0.001221\n",
      "50%                0.001274           0.002548                0.000139              0.264815                0.003443\n",
      "75%                0.001699           0.003397                0.000469              0.273774                0.011738\n",
      "max                0.130361           0.260722                0.109537              0.388614                0.445992\n"
     ]
    }
   ],
   "source": [
    "print(\"Author centrality Olympics\\n:\", author_centrality_all.describe().to_string())\n",
    "print(\"Video centrality Olympics\\n:\", video_centrality_all.describe().to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "57d7e586-2d85-4a10-a11c-7eff07b49a12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_youtube_graph_video_com(subset):\n",
    "    # Create an empty graph for the Reddit posts and comments\n",
    "    youtubeG = nx.Graph()\n",
    "\n",
    "    # Iterate through each post in the dataset\n",
    "    for video in subset:\n",
    "        if video['title']:\n",
    "            # Add the video node with attributes\n",
    "            youtubeG.add_node(video['title'], type='video', hashtag=str(video['olympics']) if video['olympics'] else \"\")\n",
    "            \n",
    "            # Add nodes and edges for each comment author and link them to the corresponding video\n",
    "            for comment in video['comments']:\n",
    "                author = comment.get('author', '').strip()\n",
    "                    \n",
    "                if author:\n",
    "                    if not youtubeG.has_node(author):\n",
    "                        youtubeG.add_node(author, type='author')\n",
    "                        \n",
    "                    # Add a directed edge from author to the video they commented on\n",
    "                    youtubeG.add_edge(author, video['title'])\n",
    "                    \n",
    "    # Perform community detection using the Louvain method\n",
    "    partition = community.best_partition(youtubeG)\n",
    "\n",
    "    nx.set_node_attributes(youtubeG, partition, 'community')\n",
    "\n",
    "    return youtubeG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "a17f9d07-542d-4a8d-99e8-d39a3205177f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interaction graph for @francisnguyen6349 exported to @francisnguyen6349_interaction_video_youtube.graphml\n"
     ]
    }
   ],
   "source": [
    "# get the most influential \n",
    "author_name = '@francisnguyen6349'\n",
    "youtubeG_select = build_youtube_graph_video_com(olympics_post2)\n",
    "partition = community.best_partition(youtubeG_select)\n",
    "\n",
    "# Find the community for the selected user\n",
    "user_community_id = partition.get(author_name)\n",
    "\n",
    "# Check if the user exists and has a community\n",
    "if user_community_id is not None:\n",
    "    user_edges = [\n",
    "        (u, v) for u, v in youtubeG_select.edges() if u == author_name or v == author_name\n",
    "    ]\n",
    "    \n",
    "    subgraph = youtubeG_select.edge_subgraph(user_edges)\n",
    "\n",
    "    output_file = f'{author_name}_interaction_video_youtube.graphml'\n",
    "    nx.write_graphml(subgraph, output_file)\n",
    "    print(f\"Interaction graph for {author_name} exported to {output_file}\")\n",
    "else:\n",
    "    print(f\"The user '{author_name}' does not exist in the graph or has no assigned community.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e10aa8ec-3bd0-4735-8961-e6ee2d66fcf4",
   "metadata": {},
   "source": [
    "### Independent Cascade for Most Active User in Youtube"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "619d7498-8aea-41d0-b0ec-4a16d457e7b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_youtube_graph_author_com(subset):\n",
    "    # Create an empty graph for the Reddit posts and comments\n",
    "    youtubeG = nx.Graph()\n",
    "\n",
    "    # Iterate through each post in the dataset\n",
    "    for video in subset:\n",
    "        if video['author']:\n",
    "            youtubeG.add_node(video['author'], type='author', hashtag=str(video['olympics']) if video['olympics'] else \"\")\n",
    "            \n",
    "            # Add nodes and edges for each comment author and link them to the corresponding video\n",
    "            for comment in video['comments']:\n",
    "                author = comment.get('author', '').strip()\n",
    "                    \n",
    "                if author:\n",
    "                    if not youtubeG.has_node(author):\n",
    "                        youtubeG.add_node(author, type='author')\n",
    "                        \n",
    "                    # Add a directed edge from author to the video they commented on\n",
    "                    youtubeG.add_edge(author, video['author'])\n",
    "                    \n",
    "    # Perform community detection using the Louvain method\n",
    "    partition = community.best_partition(youtubeG)\n",
    "\n",
    "    # Assign the community information as an attribute to each node\n",
    "    nx.set_node_attributes(youtubeG, partition, 'community')\n",
    "\n",
    "    # Return the graph with community detection information\n",
    "    return youtubeG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "3e9f43e0-096d-46d0-a45e-27b5b3e55f3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interaction graph for @francisnguyen6349 exported to @francisnguyen6349_interaction_author_youtube.graphml\n"
     ]
    }
   ],
   "source": [
    "# get the most influential \n",
    "author_name = '@francisnguyen6349'\n",
    "youtubeG_select = build_youtube_graph_author_com(olympics_post2)\n",
    "partition = community.best_partition(youtubeG_select)\n",
    "\n",
    "# Find the community for the selected user\n",
    "user_community_id = partition.get(author_name)\n",
    "\n",
    "# find the interacted posts only\n",
    "# Check if the user exists and has a community\n",
    "if user_community_id is not None:\n",
    "    user_edges = [\n",
    "        (u, v) for u, v in youtubeG_select.edges() if u == author_name or v == author_name\n",
    "    ]\n",
    "    \n",
    "    subgraph = youtubeG_select.edge_subgraph(user_edges)\n",
    "    \n",
    "    output_file = f'{author_name}_interaction_author_youtube.graphml'\n",
    "    nx.write_graphml(subgraph, output_file)\n",
    "    print(f\"Interaction graph for {author_name} exported to {output_file}\")\n",
    "else:\n",
    "    print(f\"The user '{author_name}' does not exist in the graph or has no assigned community.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "72b2116d-6199-4078-b6d1-227d17475667",
   "metadata": {},
   "outputs": [],
   "source": [
    "def independentCascade(graph, trialNum, lSeed, activationProb):\n",
    "    \"\"\"\n",
    "    Performs independent cascade over the input graph.  Results are stored in two output\n",
    "    lists.\n",
    "\n",
    "    @param graph: Input graph to perform cascade over.\n",
    "    @param trialNum: The number of runs/trials to run.  The results are averaged over the\n",
    "                    the trials/runs.\n",
    "    @param lSeed: List of initial nodes to seed.  Range from 0 to number of nodes -1.\n",
    "    @param activationProb: Activation probability on each edge.  All edges have the same\n",
    "                    activation probability.\n",
    "\n",
    "    @return: Two lists, lAvgActivationsPerNode and lAvgActivationsPerIteration.\n",
    "            lAvgActivationsPerNode is a list with the size same as the number of nodes in\n",
    "            the graph.  Each index of the list (starting with zero) corresponds directly\n",
    "            to the associated node, and each entry represents the average number of activations\n",
    "            over the trials/runs, and should lie in [0,1] range.\n",
    "            lAvgActivationsPerIteration is a list with the size same as the number of trials/runs.\n",
    "            Each index of the list corresponds to a trial/run, and each entry is the\n",
    "            total number of active nodes in that trial/run.\n",
    "    \"\"\"\n",
    "\n",
    "    # generate initial lists/vectors for the two output lists\n",
    "    lAvgActivationsPerNode = [0 for x in range(nx.number_of_nodes(graph))]\n",
    "    lAvgActivationsPerIteration = []\n",
    "\n",
    "    # Map node IDs to list indices\n",
    "    node_to_index = {node: idx for idx, node in enumerate(graph.nodes())}\n",
    "\n",
    "    print('starting cascade run')\n",
    "    # loop through the runs/trials\n",
    "    for i in range(trialNum):\n",
    "        print('Trial/run no. {}'.format(i))\n",
    "\n",
    "        #\n",
    "        # TODO: complete implemention\n",
    "        #\n",
    "        \n",
    "        # list of active nodes\n",
    "        setActive = set(lSeed)\n",
    "        setLastActive = set(lSeed)\n",
    "        setNewActive = set()\n",
    "        # we keep looping until no more new activations\n",
    "        while len(setLastActive) > 0:\n",
    "            # for each active node, we try to influence its (unactived neighbours)\n",
    "            for currNode in setLastActive:\n",
    "                # check each neighbour\n",
    "                for neighbour in graph.neighbors(currNode):\n",
    "                    # we only want non-active neighbours\n",
    "                    if neighbour not in setActive and neighbour not in setNewActive:\n",
    "                        if random.random() < activationProb:\n",
    "                            setNewActive.add(neighbour)\n",
    "\n",
    "            # update last active\n",
    "            setLastActive = setNewActive\n",
    "            # extend active set\n",
    "            setActive.update(setNewActive)\n",
    "            # reset new active\n",
    "            setNewActive = set()\n",
    "\n",
    "        # update the output lists\n",
    "        for x in setActive:\n",
    "            lAvgActivationsPerNode[node_to_index[x]] += 1\n",
    "\n",
    "        # update with total number of activations\n",
    "        lAvgActivationsPerIteration.append(len(setActive))\n",
    "\n",
    "    # placeholder, replace with appropriate returns (if necessary)\n",
    "    # we average each entry in lAvgActivationsPerNode by number of runs/trials\n",
    "    return [float(count) / trialNum for count in lAvgActivationsPerNode], lAvgActivationsPerIteration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "1ec19cd2-3fbf-4f26-89b2-74ce28176a03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting cascade run\n",
      "Trial/run no. 0\n",
      "Trial/run no. 1\n",
      "Trial/run no. 2\n",
      "Trial/run no. 3\n",
      "Trial/run no. 4\n",
      "Trial/run no. 5\n",
      "Trial/run no. 6\n",
      "Trial/run no. 7\n",
      "Trial/run no. 8\n",
      "Trial/run no. 9\n",
      "[1.0, 1.0, 0.3, 0.6, 0.8, 0.4, 0.4, 0.2, 0.7, 0.6, 0.1, 0.3, 0.5, 0.5, 0.3, 0.3]\n",
      "[7, 2, 9, 11, 10, 9, 11, 2, 10, 9]\n",
      "Average number of nodes activated = 8.0 out of 16\n",
      "starting cascade run\n",
      "Trial/run no. 0\n",
      "Trial/run no. 1\n",
      "Trial/run no. 2\n",
      "Trial/run no. 3\n",
      "Trial/run no. 4\n",
      "Trial/run no. 5\n",
      "Trial/run no. 6\n",
      "Trial/run no. 7\n",
      "Trial/run no. 8\n",
      "Trial/run no. 9\n",
      "[1.0, 1.0, 0.5, 0.4, 0.7, 0.6, 0.3, 0.6, 0.4, 0.4, 0.5, 0.3, 0.5, 0.2, 0.5, 0.2]\n",
      "[13, 11, 2, 12, 2, 10, 10, 2, 11, 8]\n",
      "Average number of nodes activated = 8.1 out of 16\n"
     ]
    }
   ],
   "source": [
    "# fileName\n",
    "author_name = '@francisnguyen6349'\n",
    "sFilenameSuffix = f'{author_name}_independent_cascade.graphml'\n",
    "\n",
    "# tree graph\n",
    "treeGraph = nx.read_graphml('@francisnguyen6349_interaction_author_youtube.graphml')\n",
    "\n",
    "# small world graph\n",
    "smallWorldGraph = nx.read_graphml('@francisnguyen6349_interaction_author_youtube.graphml')\n",
    "\n",
    "#\n",
    "# Independent cascade\n",
    "#\n",
    "lSeed = list(treeGraph.nodes())[:2]  # Get first two nodes as seeds\n",
    "trialNum = 10\n",
    "activationProb = 0.5\n",
    "\n",
    "#\n",
    "# independent cascade on tree graph\n",
    "\n",
    "if treeGraph != None:\n",
    "    lAvgActivationsPerNode, lAvgActivationsPerIteration = independentCascade(treeGraph, trialNum, lSeed, activationProb)\n",
    "    print(lAvgActivationsPerNode)\n",
    "    print(lAvgActivationsPerIteration)\n",
    "    print('Average number of nodes activated = {} out of {}'.format(sum(lAvgActivationsPerIteration) / len(lAvgActivationsPerIteration), nx.number_of_nodes(treeGraph)))\n",
    "\n",
    "\n",
    "    # Save to graph\n",
    "    # average activation per node for balanced tree,\n",
    "    # stored in node attribute 'avgAct'\n",
    "    # Save the average activation per node for balanced tree\n",
    "    # use zip and nodes since the node ids are not numbers\n",
    "    for node, avgActivation in zip(treeGraph.nodes(), lAvgActivationsPerNode):\n",
    "        treeGraph.nodes[node]['avgAct'] = avgActivation\n",
    "\n",
    "\n",
    "    # Output modified graphs to respective files\n",
    "    nx.readwrite.write_graphml(treeGraph, 'tree_' + sFilenameSuffix, infer_numeric_types=True)\n",
    "\n",
    "#\n",
    "# small world graph\n",
    "#\n",
    "\n",
    "if smallWorldGraph != None:\n",
    "    lAvgActivationsPerNode, lAvgActivationsPerIteration = independentCascade(smallWorldGraph, trialNum, lSeed, activationProb)\n",
    "    print(lAvgActivationsPerNode)\n",
    "    print(lAvgActivationsPerIteration)\n",
    "    print('Average number of nodes activated = {} out of {}'.format(sum(lAvgActivationsPerIteration) / len(lAvgActivationsPerIteration), nx.number_of_nodes(smallWorldGraph)))\n",
    "\n",
    "\n",
    "    # average activation per node for small world graph,\n",
    "    # stored in node attribute 'avgAct'\n",
    "    # use zip and nodes since the node ids are not numbers\n",
    "    for node, avgActivation in zip(smallWorldGraph.nodes(), lAvgActivationsPerNode):\n",
    "        smallWorldGraph.nodes[node]['avgAct'] = avgActivation\n",
    "\n",
    "    # Output modified graphs to respective files\n",
    "    nx.readwrite.write_graphml(smallWorldGraph, 'smallWorld_' + sFilenameSuffix, infer_numeric_types=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3e20b97-00f3-45d5-bed1-c0f341909154",
   "metadata": {},
   "source": [
    "### Linear Threshold Influence Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "ec436489-925e-47b2-bc40-4fb3ffc1199c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linearThreshold(graph, trialNum, lSeed):\n",
    "    \"\"\"\n",
    "    Performs linear threshold model over the input directed graph.  Results are stored in two output\n",
    "    lists.\n",
    "\n",
    "    @param graph: Input graph to perform the LT model over.\n",
    "    @param trialNum: The number of runs/trials to run.  The results are averaged over the\n",
    "                    the trials/runs.\n",
    "    @param lSeed: List of initial nodes to seed.  Range from 0 to number of nodes -1.\n",
    "\n",
    "    @return: Two lists, lAvgActivationsPerNode and lAvgActivationsPerIteration.\n",
    "            lAvgActivationsPerNode is a list with the size same as the number of nodes in\n",
    "            the graph.  Each index of the list (starting with zero) corresponds directly\n",
    "            to the associated node, and each entry represents the average number of activations\n",
    "            over the trials/runs, and should lie in [0,1] range.\n",
    "            lAvgActivationsPerIteration is a list with the size same as the number of trials/runs.\n",
    "            Each index of the list corresponds to a trial/run, and each entry is the\n",
    "            total number of active nodes in that trial/run.\n",
    "    \"\"\"\n",
    "\n",
    "    # generate initial lists/vectors for the two output lists\n",
    "    lAvgActivationsPerNode = {node: 0 for node in graph.nodes()}\n",
    "    lAvgActivationsPerIteration = []\n",
    "\n",
    "    # Map node IDs to list indices\n",
    "    node_to_index = {node: idx for idx, node in enumerate(graph.nodes())}\n",
    "\n",
    "    print('starting linear threshold runs')\n",
    "    # loop through the runs/trials\n",
    "    for i in range(trialNum):\n",
    "        print('Trial/run no. {}'.format(i))\n",
    "\n",
    "        # for each node, generate the random thresholds\n",
    "        for currNode, attr in graph.nodes(data=True):\n",
    "            attr['threshold'] = random.random()\n",
    "\n",
    "        # list of active nodes\n",
    "        setActive = set(lSeed)\n",
    "        setLastActive = set(lSeed)\n",
    "        setNewActive = set()\n",
    "        # we keep looping until no more new activations\n",
    "        while len(setLastActive) > 0:\n",
    "            # we get all the nodes next to the current set of active nodes\n",
    "            neighbourSet = set()\n",
    "            for activeNode in setLastActive:\n",
    "                neighbourSet.update([neighbour for neighbour in graph.successors(activeNode) if neighbour not in setActive and neighbour not in setNewActive])\n",
    "\n",
    "            # for each of these potential neighbours to be activated, test if it will be activated\n",
    "            for neighbour in neighbourSet:\n",
    "                try:\n",
    "                    # get the sum of weights\n",
    "                    weightTotal = sum([dataDict['weight'] for (u,v, dataDict) in graph.in_edges(neighbour, data=True)])\n",
    "                    # test against the node threshold\n",
    "                    if graph.nodes[neighbour]['threshold'] < weightTotal:\n",
    "                        setNewActive.add(neighbour)\n",
    "                except KeyError as e:\n",
    "                    print(\"Key error: {} is missing for edge\".format(e, (u,v)))\n",
    "\n",
    "            # update last active\n",
    "            setLastActive = setNewActive\n",
    "            # extend active set\n",
    "            setActive.update(setNewActive)\n",
    "            # reset new active\n",
    "            setNewActive = set()\n",
    "\n",
    "        # update the output lists\n",
    "        for x in setActive:\n",
    "            lAvgActivationsPerNode[x] += 1\n",
    "            \n",
    "        # update with total number of activations\n",
    "        lAvgActivationsPerIteration.append(len(setActive))\n",
    "\n",
    "    # we average each entry in lAvgActivationsPerNode by number of runs/trials\n",
    "    # Return the dictionary instead of the list\n",
    "    return {node: float(count) / trialNum for node, count in lAvgActivationsPerNode.items()}, lAvgActivationsPerIteration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a0e0183f-85e8-4572-b247-143717a27cf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateWeights(graph):\n",
    "    \"\"\"\n",
    "    Generate weights for the edges.\n",
    "\n",
    "    @param graph: directed graph to generate weights on the edges.\n",
    "    @return: modified directed graph with weights on edges, under attribute 'weight'\n",
    "    \"\"\"\n",
    "\n",
    "    for currNode in graph.nodes():\n",
    "        # generate the number that the weights should sum up to\n",
    "        totalWeight = random.random()\n",
    "        # use dirichlet distribution to generate the weights\n",
    "        aWeights = np.random.dirichlet(np.ones(graph.in_degree(currNode)), size=1) * totalWeight\n",
    "        lWeights = aWeights[0].tolist()\n",
    "\n",
    "        for i,u in enumerate(graph.predecessors(currNode)):\n",
    "            graph.add_edge(u,currNode,weight=lWeights[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "da9151a2-1295-4232-8558-9eb968139a3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting linear threshold runs\n",
      "Trial/run no. 0\n",
      "Trial/run no. 1\n",
      "Trial/run no. 2\n",
      "Trial/run no. 3\n",
      "Trial/run no. 4\n",
      "Trial/run no. 5\n",
      "Trial/run no. 6\n",
      "Trial/run no. 7\n",
      "Trial/run no. 8\n",
      "Trial/run no. 9\n",
      "{'Benedict': 1.0, 'StartPlayX': 1.0, 'Lady Gaga': 0.1, 'NeoTechnoman': 0.1, '@francisnguyen6349': 0.4, 'Olympics Aquatics': 0.0, 'KokiriGaming': 0.4, 'ProsafiaGaming': 0.3, 'Paralympic Games': 0.4, 'Olympics': 0.1, 'JinnaGaming': 0.0, 'NBC Sports': 0.2, 'Games Variety': 0.0, 'Nintendo Utopia': 0.4, 'Olympics Gymnastics': 0.1, 'Zacnary': 0.3}\n",
      "[9, 2, 2, 2, 9, 2, 9, 2, 2, 9]\n",
      "Average number of nodes activated = 4.8 out of 16\n",
      "starting linear threshold runs\n",
      "Trial/run no. 0\n",
      "Trial/run no. 1\n",
      "Trial/run no. 2\n",
      "Trial/run no. 3\n",
      "Trial/run no. 4\n",
      "Trial/run no. 5\n",
      "Trial/run no. 6\n",
      "Trial/run no. 7\n",
      "Trial/run no. 8\n",
      "Trial/run no. 9\n",
      "{'Benedict': 1.0, 'StartPlayX': 1.0, 'Lady Gaga': 0.5, 'NeoTechnoman': 0.4, '@francisnguyen6349': 0.8, 'Olympics Aquatics': 0.5, 'KokiriGaming': 0.5, 'ProsafiaGaming': 0.4, 'Paralympic Games': 0.3, 'Olympics': 0.5, 'JinnaGaming': 0.3, 'NBC Sports': 0.1, 'Games Variety': 0.0, 'Nintendo Utopia': 0.6, 'Olympics Gymnastics': 0.3, 'Zacnary': 0.0}\n",
      "[10, 2, 6, 10, 9, 2, 8, 8, 8, 9]\n",
      "Average number of nodes activated = 7.2 out of 16\n"
     ]
    }
   ],
   "source": [
    "# fileName\n",
    "author_name = '@francisnguyen6349'\n",
    "sFilenameSuffix = f'{author_name}_linear_threshold.graphml'\n",
    "\n",
    "# tree graph\n",
    "undirectedTreeGraph = nx.read_graphml('@francisnguyen6349_interaction_author_youtube.graphml')\n",
    "treeGraph = undirectedTreeGraph.to_directed()\n",
    "generateWeights(treeGraph)\n",
    "\n",
    "\n",
    "# small world graph\n",
    "undirectedSmallWorldGraph = nx.read_graphml('@francisnguyen6349_interaction_author_youtube.graphml')\n",
    "smallWorldGraph = undirectedSmallWorldGraph.to_directed()\n",
    "generateWeights(smallWorldGraph)\n",
    "\n",
    "\n",
    "#\n",
    "# Linear threshold\n",
    "#\n",
    "seedNum = 3\n",
    "lSeed = list(treeGraph.nodes())[:2]  # Get first two nodes as seeds\n",
    "trialNum = 10\n",
    "\n",
    "\n",
    "#\n",
    "# TODO: complete the implementation of the linear threshold model with function\n",
    "# linearThreshold()\n",
    "#\n",
    "\n",
    "#\n",
    "# tree graph\n",
    "#\n",
    "\n",
    "if treeGraph != None:\n",
    "    lAvgActivationsPerNode, lAvgActivationsPerIteration = linearThreshold(treeGraph, trialNum, lSeed)\n",
    "    print(lAvgActivationsPerNode)\n",
    "    print(lAvgActivationsPerIteration)\n",
    "    if len(lAvgActivationsPerIteration) > 0:\n",
    "        print('Average number of nodes activated = {} out of {}'.format(sum(lAvgActivationsPerIteration) / len(lAvgActivationsPerIteration), nx.number_of_nodes(treeGraph)))\n",
    "    else:\n",
    "        print('Average number of nodes activated = {} out of {}'.format(0, nx.number_of_nodes(treeGraph)))\n",
    "\n",
    "\n",
    "    # average activation per node for small world graph,\n",
    "    # stored in node attribute 'avgAct'\n",
    "    for nodeId, avgActivation in lAvgActivationsPerNode.items():\n",
    "        treeGraph.nodes[nodeId]['avgAct'] = avgActivation\n",
    "\n",
    "    # Output modified graphs to respective files\n",
    "    nx.readwrite.write_graphml(treeGraph, 'treeLT_' + sFilenameSuffix, infer_numeric_types=True)\n",
    "\n",
    "#\n",
    "# small world graph\n",
    "#\n",
    "\n",
    "if smallWorldGraph != None:\n",
    "    lAvgActivationsPerNode, lAvgActivationsPerIteration = linearThreshold(smallWorldGraph, trialNum, lSeed)\n",
    "    print(lAvgActivationsPerNode)\n",
    "    print(lAvgActivationsPerIteration)\n",
    "    if len(lAvgActivationsPerIteration) > 0:\n",
    "        print('Average number of nodes activated = {} out of {}'.format(sum(lAvgActivationsPerIteration) / len(lAvgActivationsPerIteration), nx.number_of_nodes(smallWorldGraph)))\n",
    "    else:\n",
    "        print('Average number of nodes activated = {} out of {}'.format(0, nx.number_of_nodes(smallWorldGraph)))\n",
    "\n",
    "\n",
    "    # average activation per node for small world graph,\n",
    "    # stored in node attribute 'avgAct'\n",
    "    for nodeId, avgActivation in lAvgActivationsPerNode.items():\n",
    "        smallWorldGraph.nodes[nodeId]['avgAct'] = avgActivation\n",
    "\n",
    "    # Output modified graphs to respective files\n",
    "    nx.readwrite.write_graphml(smallWorldGraph, 'smallWorldLT_' + sFilenameSuffix, infer_numeric_types=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b089399-ae66-4c34-bc10-ac87d2c916b9",
   "metadata": {},
   "source": [
    "### Mean of Centrality Measures Across Olympic Events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "605aaf7a-5ba0-4836-a503-4bca2252ea65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For author centrality:\n",
      "                        Olympics 2016  Olympics 2020  Olympics 2024\n",
      "in_degree_centrality         0.318182       0.040969       0.028292\n",
      "degree_centrality            0.636364       0.081939       0.056584\n",
      "betweenness_centrality       0.092727       0.016773       0.010571\n",
      "closeness_centrality         0.529180       0.342183       0.361191\n",
      "eigenvector_centrality       0.276653       0.064549       0.056079 \n",
      "\n",
      "For video centrality:\n",
      "                        Olympics 2016  Olympics 2020  Olympics 2024\n",
      "in_degree_centrality         0.040464       0.006229       0.010676\n",
      "degree_centrality            0.080929       0.012458       0.021353\n",
      "betweenness_centrality       0.014948       0.002639       0.005123\n",
      "closeness_centrality         0.323899       0.283770       0.275410\n",
      "eigenvector_centrality       0.067797       0.017461       0.028301\n"
     ]
    }
   ],
   "source": [
    "# author\n",
    "author_centrality_2016_mean = author_centrality_2016.mean()\n",
    "author_centrality_2020_mean = author_centrality_2020.mean()\n",
    "author_centrality_2024_mean = author_centrality_2024.mean()\n",
    "all_author_centrality_means = pd.concat([author_centrality_2016_mean, author_centrality_2020_mean, author_centrality_2024_mean], axis=1)\n",
    "all_author_centrality_means = all_author_centrality_means.rename(columns={0: 'Olympics 2016', 1: 'Olympics 2020', 2: 'Olympics 2024'})\n",
    "\n",
    "\n",
    "# video\n",
    "video_centrality_2016_mean = video_centrality_2016.mean()\n",
    "video_centrality_2020_mean = video_centrality_2020.mean()\n",
    "video_centrality_2024_mean = video_centrality_2024.mean()\n",
    "all_video_centrality_means = pd.concat([video_centrality_2016_mean, video_centrality_2020_mean, video_centrality_2024_mean], axis=1)\n",
    "all_video_centrality_means = all_video_centrality_means.rename(columns={0: 'Olympics 2016', 1: 'Olympics 2020', 2: 'Olympics 2024'})\n",
    "\n",
    "print('For author centrality:')\n",
    "print(all_author_centrality_means, '\\n')\n",
    "print('For video centrality:')\n",
    "print(all_video_centrality_means)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6f05d373-4c7e-4c94-91e2-827fff57257e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>in_degree_centrality</th>\n",
       "      <th>degree_centrality</th>\n",
       "      <th>betweenness_centrality</th>\n",
       "      <th>closeness_centrality</th>\n",
       "      <th>eigenvector_centrality</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Olympics</th>\n",
       "      <td>0.636364</td>\n",
       "      <td>1.272727</td>\n",
       "      <td>0.380407</td>\n",
       "      <td>0.733333</td>\n",
       "      <td>0.484901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>@tekkenfan01</th>\n",
       "      <td>0.272727</td>\n",
       "      <td>0.545455</td>\n",
       "      <td>0.066121</td>\n",
       "      <td>0.523810</td>\n",
       "      <td>0.273509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>@93hothead</th>\n",
       "      <td>0.272727</td>\n",
       "      <td>0.545455</td>\n",
       "      <td>0.029082</td>\n",
       "      <td>0.523810</td>\n",
       "      <td>0.291544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>@1990Thunderbolt</th>\n",
       "      <td>0.272727</td>\n",
       "      <td>0.545455</td>\n",
       "      <td>0.029082</td>\n",
       "      <td>0.523810</td>\n",
       "      <td>0.291544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>The NZ Team</th>\n",
       "      <td>0.181818</td>\n",
       "      <td>0.363636</td>\n",
       "      <td>0.015195</td>\n",
       "      <td>0.407407</td>\n",
       "      <td>0.125543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>@stt5v2002</th>\n",
       "      <td>0.272727</td>\n",
       "      <td>0.545455</td>\n",
       "      <td>0.086190</td>\n",
       "      <td>0.523810</td>\n",
       "      <td>0.230756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>@redzoom7857</th>\n",
       "      <td>0.272727</td>\n",
       "      <td>0.545455</td>\n",
       "      <td>0.108000</td>\n",
       "      <td>0.523810</td>\n",
       "      <td>0.252133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>@digitalhouse6969</th>\n",
       "      <td>0.272727</td>\n",
       "      <td>0.545455</td>\n",
       "      <td>0.066121</td>\n",
       "      <td>0.523810</td>\n",
       "      <td>0.273509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Jomboy Media</th>\n",
       "      <td>0.363636</td>\n",
       "      <td>0.727273</td>\n",
       "      <td>0.105455</td>\n",
       "      <td>0.523810</td>\n",
       "      <td>0.277136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>@longlee1100</th>\n",
       "      <td>0.272727</td>\n",
       "      <td>0.545455</td>\n",
       "      <td>0.062675</td>\n",
       "      <td>0.523810</td>\n",
       "      <td>0.252133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Vox</th>\n",
       "      <td>0.272727</td>\n",
       "      <td>0.545455</td>\n",
       "      <td>0.026407</td>\n",
       "      <td>0.440000</td>\n",
       "      <td>0.207765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Business Insider</th>\n",
       "      <td>0.454545</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>0.137991</td>\n",
       "      <td>0.578947</td>\n",
       "      <td>0.359358</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   in_degree_centrality  degree_centrality  \\\n",
       "Olympics                       0.636364           1.272727   \n",
       "@tekkenfan01                   0.272727           0.545455   \n",
       "@93hothead                     0.272727           0.545455   \n",
       "@1990Thunderbolt               0.272727           0.545455   \n",
       "The NZ Team                    0.181818           0.363636   \n",
       "@stt5v2002                     0.272727           0.545455   \n",
       "@redzoom7857                   0.272727           0.545455   \n",
       "@digitalhouse6969              0.272727           0.545455   \n",
       "Jomboy Media                   0.363636           0.727273   \n",
       "@longlee1100                   0.272727           0.545455   \n",
       "Vox                            0.272727           0.545455   \n",
       "Business Insider               0.454545           0.909091   \n",
       "\n",
       "                   betweenness_centrality  closeness_centrality  \\\n",
       "Olympics                         0.380407              0.733333   \n",
       "@tekkenfan01                     0.066121              0.523810   \n",
       "@93hothead                       0.029082              0.523810   \n",
       "@1990Thunderbolt                 0.029082              0.523810   \n",
       "The NZ Team                      0.015195              0.407407   \n",
       "@stt5v2002                       0.086190              0.523810   \n",
       "@redzoom7857                     0.108000              0.523810   \n",
       "@digitalhouse6969                0.066121              0.523810   \n",
       "Jomboy Media                     0.105455              0.523810   \n",
       "@longlee1100                     0.062675              0.523810   \n",
       "Vox                              0.026407              0.440000   \n",
       "Business Insider                 0.137991              0.578947   \n",
       "\n",
       "                   eigenvector_centrality  \n",
       "Olympics                         0.484901  \n",
       "@tekkenfan01                     0.273509  \n",
       "@93hothead                       0.291544  \n",
       "@1990Thunderbolt                 0.291544  \n",
       "The NZ Team                      0.125543  \n",
       "@stt5v2002                       0.230756  \n",
       "@redzoom7857                     0.252133  \n",
       "@digitalhouse6969                0.273509  \n",
       "Jomboy Media                     0.277136  \n",
       "@longlee1100                     0.252133  \n",
       "Vox                              0.207765  \n",
       "Business Insider                 0.359358  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "author_centrality_2016"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "620d5ca0-a233-40c8-bbdb-8ffdf3f10e1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>in_degree_centrality</th>\n",
       "      <th>degree_centrality</th>\n",
       "      <th>betweenness_centrality</th>\n",
       "      <th>closeness_centrality</th>\n",
       "      <th>eigenvector_centrality</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Olympics</th>\n",
       "      <td>0.472441</td>\n",
       "      <td>0.944882</td>\n",
       "      <td>0.369497</td>\n",
       "      <td>0.533613</td>\n",
       "      <td>0.529522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>@kankeihatsu291</th>\n",
       "      <td>0.023622</td>\n",
       "      <td>0.047244</td>\n",
       "      <td>0.002686</td>\n",
       "      <td>0.362857</td>\n",
       "      <td>0.070549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>@cirnosnumberfan6449</th>\n",
       "      <td>0.023622</td>\n",
       "      <td>0.047244</td>\n",
       "      <td>0.010733</td>\n",
       "      <td>0.389571</td>\n",
       "      <td>0.060372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>@manojkumar-xl9ez</th>\n",
       "      <td>0.023622</td>\n",
       "      <td>0.047244</td>\n",
       "      <td>0.000166</td>\n",
       "      <td>0.352778</td>\n",
       "      <td>0.098099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>@rhapsody8883</th>\n",
       "      <td>0.023622</td>\n",
       "      <td>0.047244</td>\n",
       "      <td>0.000166</td>\n",
       "      <td>0.352778</td>\n",
       "      <td>0.098099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ZerkaaPlays</th>\n",
       "      <td>0.023622</td>\n",
       "      <td>0.047244</td>\n",
       "      <td>0.001090</td>\n",
       "      <td>0.317500</td>\n",
       "      <td>0.016058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Namewee</th>\n",
       "      <td>0.070866</td>\n",
       "      <td>0.141732</td>\n",
       "      <td>0.009700</td>\n",
       "      <td>0.317500</td>\n",
       "      <td>0.033897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>@aaliyahrosado7365</th>\n",
       "      <td>0.023622</td>\n",
       "      <td>0.047244</td>\n",
       "      <td>0.004194</td>\n",
       "      <td>0.279736</td>\n",
       "      <td>0.009153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Cody Miller</th>\n",
       "      <td>0.015748</td>\n",
       "      <td>0.031496</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.262397</td>\n",
       "      <td>0.016116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>The B1M</th>\n",
       "      <td>0.125984</td>\n",
       "      <td>0.251969</td>\n",
       "      <td>0.036447</td>\n",
       "      <td>0.362857</td>\n",
       "      <td>0.096239</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>128 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      in_degree_centrality  degree_centrality  \\\n",
       "Olympics                          0.472441           0.944882   \n",
       "@kankeihatsu291                   0.023622           0.047244   \n",
       "@cirnosnumberfan6449              0.023622           0.047244   \n",
       "@manojkumar-xl9ez                 0.023622           0.047244   \n",
       "@rhapsody8883                     0.023622           0.047244   \n",
       "...                                    ...                ...   \n",
       "ZerkaaPlays                       0.023622           0.047244   \n",
       "Namewee                           0.070866           0.141732   \n",
       "@aaliyahrosado7365                0.023622           0.047244   \n",
       "Cody Miller                       0.015748           0.031496   \n",
       "The B1M                           0.125984           0.251969   \n",
       "\n",
       "                      betweenness_centrality  closeness_centrality  \\\n",
       "Olympics                            0.369497              0.533613   \n",
       "@kankeihatsu291                     0.002686              0.362857   \n",
       "@cirnosnumberfan6449                0.010733              0.389571   \n",
       "@manojkumar-xl9ez                   0.000166              0.352778   \n",
       "@rhapsody8883                       0.000166              0.352778   \n",
       "...                                      ...                   ...   \n",
       "ZerkaaPlays                         0.001090              0.317500   \n",
       "Namewee                             0.009700              0.317500   \n",
       "@aaliyahrosado7365                  0.004194              0.279736   \n",
       "Cody Miller                         0.000000              0.262397   \n",
       "The B1M                             0.036447              0.362857   \n",
       "\n",
       "                      eigenvector_centrality  \n",
       "Olympics                            0.529522  \n",
       "@kankeihatsu291                     0.070549  \n",
       "@cirnosnumberfan6449                0.060372  \n",
       "@manojkumar-xl9ez                   0.098099  \n",
       "@rhapsody8883                       0.098099  \n",
       "...                                      ...  \n",
       "ZerkaaPlays                         0.016058  \n",
       "Namewee                             0.033897  \n",
       "@aaliyahrosado7365                  0.009153  \n",
       "Cody Miller                         0.016116  \n",
       "The B1M                             0.096239  \n",
       "\n",
       "[128 rows x 5 columns]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "author_centrality_2020"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "491d72f9-a783-49e6-8d1a-a7c26ee7953e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>in_degree_centrality</th>\n",
       "      <th>degree_centrality</th>\n",
       "      <th>betweenness_centrality</th>\n",
       "      <th>closeness_centrality</th>\n",
       "      <th>eigenvector_centrality</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Olympics</th>\n",
       "      <td>0.519337</td>\n",
       "      <td>1.038674</td>\n",
       "      <td>0.538946</td>\n",
       "      <td>0.560372</td>\n",
       "      <td>0.527974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>@francisnguyen6349</th>\n",
       "      <td>0.016575</td>\n",
       "      <td>0.033149</td>\n",
       "      <td>0.001620</td>\n",
       "      <td>0.410431</td>\n",
       "      <td>0.074895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>@tasmanndrive</th>\n",
       "      <td>0.016575</td>\n",
       "      <td>0.033149</td>\n",
       "      <td>0.022497</td>\n",
       "      <td>0.368635</td>\n",
       "      <td>0.046639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>@awaleahmed8698</th>\n",
       "      <td>0.016575</td>\n",
       "      <td>0.033149</td>\n",
       "      <td>0.028291</td>\n",
       "      <td>0.385928</td>\n",
       "      <td>0.058350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BBC</th>\n",
       "      <td>0.044199</td>\n",
       "      <td>0.088398</td>\n",
       "      <td>0.001936</td>\n",
       "      <td>0.327306</td>\n",
       "      <td>0.026479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dridri</th>\n",
       "      <td>0.016575</td>\n",
       "      <td>0.033149</td>\n",
       "      <td>0.000104</td>\n",
       "      <td>0.301165</td>\n",
       "      <td>0.010825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Vox</th>\n",
       "      <td>0.033149</td>\n",
       "      <td>0.066298</td>\n",
       "      <td>0.000937</td>\n",
       "      <td>0.324955</td>\n",
       "      <td>0.023642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TFC Stadiums</th>\n",
       "      <td>0.033149</td>\n",
       "      <td>0.066298</td>\n",
       "      <td>0.000462</td>\n",
       "      <td>0.318102</td>\n",
       "      <td>0.032692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Firstpost</th>\n",
       "      <td>0.154696</td>\n",
       "      <td>0.309392</td>\n",
       "      <td>0.130162</td>\n",
       "      <td>0.370143</td>\n",
       "      <td>0.132324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Planet Canoe</th>\n",
       "      <td>0.005525</td>\n",
       "      <td>0.011050</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.234760</td>\n",
       "      <td>0.002247</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>182 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    in_degree_centrality  degree_centrality  \\\n",
       "Olympics                        0.519337           1.038674   \n",
       "@francisnguyen6349              0.016575           0.033149   \n",
       "@tasmanndrive                   0.016575           0.033149   \n",
       "@awaleahmed8698                 0.016575           0.033149   \n",
       "BBC                             0.044199           0.088398   \n",
       "...                                  ...                ...   \n",
       "dridri                          0.016575           0.033149   \n",
       "Vox                             0.033149           0.066298   \n",
       "TFC Stadiums                    0.033149           0.066298   \n",
       "Firstpost                       0.154696           0.309392   \n",
       "Planet Canoe                    0.005525           0.011050   \n",
       "\n",
       "                    betweenness_centrality  closeness_centrality  \\\n",
       "Olympics                          0.538946              0.560372   \n",
       "@francisnguyen6349                0.001620              0.410431   \n",
       "@tasmanndrive                     0.022497              0.368635   \n",
       "@awaleahmed8698                   0.028291              0.385928   \n",
       "BBC                               0.001936              0.327306   \n",
       "...                                    ...                   ...   \n",
       "dridri                            0.000104              0.301165   \n",
       "Vox                               0.000937              0.324955   \n",
       "TFC Stadiums                      0.000462              0.318102   \n",
       "Firstpost                         0.130162              0.370143   \n",
       "Planet Canoe                      0.000000              0.234760   \n",
       "\n",
       "                    eigenvector_centrality  \n",
       "Olympics                          0.527974  \n",
       "@francisnguyen6349                0.074895  \n",
       "@tasmanndrive                     0.046639  \n",
       "@awaleahmed8698                   0.058350  \n",
       "BBC                               0.026479  \n",
       "...                                    ...  \n",
       "dridri                            0.010825  \n",
       "Vox                               0.023642  \n",
       "TFC Stadiums                      0.032692  \n",
       "Firstpost                         0.132324  \n",
       "Planet Canoe                      0.002247  \n",
       "\n",
       "[182 rows x 5 columns]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "author_centrality_2024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "62815790-14b5-4ca2-bef9-7a48b8ba0e73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>in_degree_centrality</th>\n",
       "      <th>degree_centrality</th>\n",
       "      <th>betweenness_centrality</th>\n",
       "      <th>closeness_centrality</th>\n",
       "      <th>eigenvector_centrality</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Olympics</th>\n",
       "      <td>0.614737</td>\n",
       "      <td>1.229474</td>\n",
       "      <td>0.717285</td>\n",
       "      <td>0.610540</td>\n",
       "      <td>0.621084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NBC Sports</th>\n",
       "      <td>0.183158</td>\n",
       "      <td>0.366316</td>\n",
       "      <td>0.081397</td>\n",
       "      <td>0.388072</td>\n",
       "      <td>0.149667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Olympics Aquatics</th>\n",
       "      <td>0.128421</td>\n",
       "      <td>0.256842</td>\n",
       "      <td>0.034025</td>\n",
       "      <td>0.364823</td>\n",
       "      <td>0.120252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Yahoo Australia</th>\n",
       "      <td>0.120000</td>\n",
       "      <td>0.240000</td>\n",
       "      <td>0.014776</td>\n",
       "      <td>0.350812</td>\n",
       "      <td>0.095717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BBC</th>\n",
       "      <td>0.105263</td>\n",
       "      <td>0.210526</td>\n",
       "      <td>0.018527</td>\n",
       "      <td>0.359304</td>\n",
       "      <td>0.081436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GMA Sports PH</th>\n",
       "      <td>0.002105</td>\n",
       "      <td>0.004211</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.222378</td>\n",
       "      <td>0.000522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Tehrian Nona</th>\n",
       "      <td>0.002105</td>\n",
       "      <td>0.004211</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.276163</td>\n",
       "      <td>0.001735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NeoTechnoman</th>\n",
       "      <td>0.002105</td>\n",
       "      <td>0.004211</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.307245</td>\n",
       "      <td>0.003182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Victory Vibez</th>\n",
       "      <td>0.002105</td>\n",
       "      <td>0.004211</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.287879</td>\n",
       "      <td>0.002114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>The Wall Street Journal</th>\n",
       "      <td>0.002105</td>\n",
       "      <td>0.004211</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.281732</td>\n",
       "      <td>0.001802</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>476 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                         in_degree_centrality  degree_centrality  \\\n",
       "Olympics                             0.614737           1.229474   \n",
       "NBC Sports                           0.183158           0.366316   \n",
       "Olympics Aquatics                    0.128421           0.256842   \n",
       "Yahoo Australia                      0.120000           0.240000   \n",
       "BBC                                  0.105263           0.210526   \n",
       "...                                       ...                ...   \n",
       "GMA Sports PH                        0.002105           0.004211   \n",
       "Tehrian Nona                         0.002105           0.004211   \n",
       "NeoTechnoman                         0.002105           0.004211   \n",
       "Victory Vibez                        0.002105           0.004211   \n",
       "The Wall Street Journal              0.002105           0.004211   \n",
       "\n",
       "                         betweenness_centrality  closeness_centrality  \\\n",
       "Olympics                               0.717285              0.610540   \n",
       "NBC Sports                             0.081397              0.388072   \n",
       "Olympics Aquatics                      0.034025              0.364823   \n",
       "Yahoo Australia                        0.014776              0.350812   \n",
       "BBC                                    0.018527              0.359304   \n",
       "...                                         ...                   ...   \n",
       "GMA Sports PH                          0.000000              0.222378   \n",
       "Tehrian Nona                           0.000000              0.276163   \n",
       "NeoTechnoman                           0.000000              0.307245   \n",
       "Victory Vibez                          0.000000              0.287879   \n",
       "The Wall Street Journal                0.000000              0.281732   \n",
       "\n",
       "                         eigenvector_centrality  \n",
       "Olympics                               0.621084  \n",
       "NBC Sports                             0.149667  \n",
       "Olympics Aquatics                      0.120252  \n",
       "Yahoo Australia                        0.095717  \n",
       "BBC                                    0.081436  \n",
       "...                                         ...  \n",
       "GMA Sports PH                          0.000522  \n",
       "Tehrian Nona                           0.001735  \n",
       "NeoTechnoman                           0.003182  \n",
       "Victory Vibez                          0.002114  \n",
       "The Wall Street Journal                0.001802  \n",
       "\n",
       "[476 rows x 5 columns]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "author_centrality_all.sort_values(by = ['degree_centrality'], ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "23153172-e864-4dc3-a1ac-0862dd6ecf27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>in_degree_centrality</th>\n",
       "      <th>degree_centrality</th>\n",
       "      <th>betweenness_centrality</th>\n",
       "      <th>closeness_centrality</th>\n",
       "      <th>eigenvector_centrality</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Olympics</th>\n",
       "      <td>0.614737</td>\n",
       "      <td>1.229474</td>\n",
       "      <td>0.717285</td>\n",
       "      <td>0.610540</td>\n",
       "      <td>0.621084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NBC Sports</th>\n",
       "      <td>0.183158</td>\n",
       "      <td>0.366316</td>\n",
       "      <td>0.081397</td>\n",
       "      <td>0.388072</td>\n",
       "      <td>0.149667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Olympics Aquatics</th>\n",
       "      <td>0.128421</td>\n",
       "      <td>0.256842</td>\n",
       "      <td>0.034025</td>\n",
       "      <td>0.364823</td>\n",
       "      <td>0.120252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Yahoo Australia</th>\n",
       "      <td>0.120000</td>\n",
       "      <td>0.240000</td>\n",
       "      <td>0.014776</td>\n",
       "      <td>0.350812</td>\n",
       "      <td>0.095717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BBC</th>\n",
       "      <td>0.105263</td>\n",
       "      <td>0.210526</td>\n",
       "      <td>0.018527</td>\n",
       "      <td>0.359304</td>\n",
       "      <td>0.081436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GMA Sports PH</th>\n",
       "      <td>0.002105</td>\n",
       "      <td>0.004211</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.222378</td>\n",
       "      <td>0.000522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Tehrian Nona</th>\n",
       "      <td>0.002105</td>\n",
       "      <td>0.004211</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.276163</td>\n",
       "      <td>0.001735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NeoTechnoman</th>\n",
       "      <td>0.002105</td>\n",
       "      <td>0.004211</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.307245</td>\n",
       "      <td>0.003182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Victory Vibez</th>\n",
       "      <td>0.002105</td>\n",
       "      <td>0.004211</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.287879</td>\n",
       "      <td>0.002114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>The Wall Street Journal</th>\n",
       "      <td>0.002105</td>\n",
       "      <td>0.004211</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.281732</td>\n",
       "      <td>0.001802</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>476 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                         in_degree_centrality  degree_centrality  \\\n",
       "Olympics                             0.614737           1.229474   \n",
       "NBC Sports                           0.183158           0.366316   \n",
       "Olympics Aquatics                    0.128421           0.256842   \n",
       "Yahoo Australia                      0.120000           0.240000   \n",
       "BBC                                  0.105263           0.210526   \n",
       "...                                       ...                ...   \n",
       "GMA Sports PH                        0.002105           0.004211   \n",
       "Tehrian Nona                         0.002105           0.004211   \n",
       "NeoTechnoman                         0.002105           0.004211   \n",
       "Victory Vibez                        0.002105           0.004211   \n",
       "The Wall Street Journal              0.002105           0.004211   \n",
       "\n",
       "                         betweenness_centrality  closeness_centrality  \\\n",
       "Olympics                               0.717285              0.610540   \n",
       "NBC Sports                             0.081397              0.388072   \n",
       "Olympics Aquatics                      0.034025              0.364823   \n",
       "Yahoo Australia                        0.014776              0.350812   \n",
       "BBC                                    0.018527              0.359304   \n",
       "...                                         ...                   ...   \n",
       "GMA Sports PH                          0.000000              0.222378   \n",
       "Tehrian Nona                           0.000000              0.276163   \n",
       "NeoTechnoman                           0.000000              0.307245   \n",
       "Victory Vibez                          0.000000              0.287879   \n",
       "The Wall Street Journal                0.000000              0.281732   \n",
       "\n",
       "                         eigenvector_centrality  \n",
       "Olympics                               0.621084  \n",
       "NBC Sports                             0.149667  \n",
       "Olympics Aquatics                      0.120252  \n",
       "Yahoo Australia                        0.095717  \n",
       "BBC                                    0.081436  \n",
       "...                                         ...  \n",
       "GMA Sports PH                          0.000522  \n",
       "Tehrian Nona                           0.001735  \n",
       "NeoTechnoman                           0.003182  \n",
       "Victory Vibez                          0.002114  \n",
       "The Wall Street Journal                0.001802  \n",
       "\n",
       "[476 rows x 5 columns]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "author_centrality_all.sort_values(by = ['in_degree_centrality'], ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ee46f3eb-3ed3-4e19-85f6-bc2003521c39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>in_degree_centrality</th>\n",
       "      <th>degree_centrality</th>\n",
       "      <th>betweenness_centrality</th>\n",
       "      <th>closeness_centrality</th>\n",
       "      <th>eigenvector_centrality</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Olympics</th>\n",
       "      <td>0.614737</td>\n",
       "      <td>1.229474</td>\n",
       "      <td>0.717285</td>\n",
       "      <td>0.610540</td>\n",
       "      <td>0.621084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Zephiel810</th>\n",
       "      <td>0.033684</td>\n",
       "      <td>0.067368</td>\n",
       "      <td>0.111640</td>\n",
       "      <td>0.317089</td>\n",
       "      <td>0.011598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NBC Sports</th>\n",
       "      <td>0.183158</td>\n",
       "      <td>0.366316</td>\n",
       "      <td>0.081397</td>\n",
       "      <td>0.388072</td>\n",
       "      <td>0.149667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Lady Gaga</th>\n",
       "      <td>0.092632</td>\n",
       "      <td>0.185263</td>\n",
       "      <td>0.076591</td>\n",
       "      <td>0.356607</td>\n",
       "      <td>0.070036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>@francisnguyen6349</th>\n",
       "      <td>0.031579</td>\n",
       "      <td>0.063158</td>\n",
       "      <td>0.068557</td>\n",
       "      <td>0.443097</td>\n",
       "      <td>0.060837</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sports Today</th>\n",
       "      <td>0.004211</td>\n",
       "      <td>0.008421</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.292128</td>\n",
       "      <td>0.004042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GMA Sports PH</th>\n",
       "      <td>0.002105</td>\n",
       "      <td>0.004211</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.222378</td>\n",
       "      <td>0.000522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7thave</th>\n",
       "      <td>0.006316</td>\n",
       "      <td>0.012632</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.286490</td>\n",
       "      <td>0.005730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NeoTechnoman</th>\n",
       "      <td>0.002105</td>\n",
       "      <td>0.004211</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.307245</td>\n",
       "      <td>0.003182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>The Wall Street Journal</th>\n",
       "      <td>0.002105</td>\n",
       "      <td>0.004211</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.281732</td>\n",
       "      <td>0.001802</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>476 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                         in_degree_centrality  degree_centrality  \\\n",
       "Olympics                             0.614737           1.229474   \n",
       "Zephiel810                           0.033684           0.067368   \n",
       "NBC Sports                           0.183158           0.366316   \n",
       "Lady Gaga                            0.092632           0.185263   \n",
       "@francisnguyen6349                   0.031579           0.063158   \n",
       "...                                       ...                ...   \n",
       "Sports Today                         0.004211           0.008421   \n",
       "GMA Sports PH                        0.002105           0.004211   \n",
       "7thave                               0.006316           0.012632   \n",
       "NeoTechnoman                         0.002105           0.004211   \n",
       "The Wall Street Journal              0.002105           0.004211   \n",
       "\n",
       "                         betweenness_centrality  closeness_centrality  \\\n",
       "Olympics                               0.717285              0.610540   \n",
       "Zephiel810                             0.111640              0.317089   \n",
       "NBC Sports                             0.081397              0.388072   \n",
       "Lady Gaga                              0.076591              0.356607   \n",
       "@francisnguyen6349                     0.068557              0.443097   \n",
       "...                                         ...                   ...   \n",
       "Sports Today                           0.000000              0.292128   \n",
       "GMA Sports PH                          0.000000              0.222378   \n",
       "7thave                                 0.000000              0.286490   \n",
       "NeoTechnoman                           0.000000              0.307245   \n",
       "The Wall Street Journal                0.000000              0.281732   \n",
       "\n",
       "                         eigenvector_centrality  \n",
       "Olympics                               0.621084  \n",
       "Zephiel810                             0.011598  \n",
       "NBC Sports                             0.149667  \n",
       "Lady Gaga                              0.070036  \n",
       "@francisnguyen6349                     0.060837  \n",
       "...                                         ...  \n",
       "Sports Today                           0.004042  \n",
       "GMA Sports PH                          0.000522  \n",
       "7thave                                 0.005730  \n",
       "NeoTechnoman                           0.003182  \n",
       "The Wall Street Journal                0.001802  \n",
       "\n",
       "[476 rows x 5 columns]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "author_centrality_all.sort_values(by = ['betweenness_centrality'], ascending = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
